---
layout: post
title: "Performance Testing"
excerpt: "Why do performance testing? And how"
tags: [perftest]
shorturl: "https://goo.gl/wNVxWD"
comments: true
---
<i>{{ page.excerpt }}</i>
<hr />

{% include _toc.html %}

This is about the economics of performance testing and performance engineering work.

## For executives

Performance testing is done to:

   * ensure productivity of employees 
   * prevent losing sales from vistors abandoning interactions
   * avoid damaged to the company's reputation 

## For strategic program managers

Spending money on performance testing is like companies paying for pre-employment drug tests.
All employers inconvenience new hires with 
drug testing even though the vast majority of employees don't use drugs because
the consequences of that one bad apple potentially devastating trust among employees.

Just as there are several types of drugs potential employees can be using, 
there are different <strong>performance risks</strong> that may undermine enterprise systems.

Various types of performance testing address different concerns:

1. Where can bad programming "hang" the system?

   For the same reason as 100% of candidates are made to take drug tests,
   the more <strong>coverage</strong> of user functionality in performance tests, 
   the more likely we are to find hidden issues.

   "Performance tests" are done to identify how quickly servers respond
   to requests from customers and employees. The measurement is called
   <strong>response time</strong>, expressed in seconds or milliseconds (thousands of a second).

   The "Shift-left" movement to catch performance problems earlier involves
   watching out for slow performance while developers create the system.
   
   As more and more processing moves to software on devices rather than on servers,
   many organizations now measure how much time programs take to "render" data obtained from servers.


2. What is the <strong>capacity</strong> of the system?<br />
   What is the highest rate the system can process business transactions?

   Some servers work until a certain level of work burns them out.
   It is always helpful to have a statement of the likely peak number of users that might be expected to use the system at peak times. 

   <strong>Peak</strong> demand in a system is defined practically by the
   <strong>number of users</strong> who are actively using the system at the same time
   (concurrently).

   "Load tests" are conducted to identify the rate business transactions can be processed without failure
   at various levels of expected load (demand from users).

   "Stress tests" reveals how much servers can handle before they run out of 
   memory, processing power, disk space, or other resource needed.

   Stress testing is normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system's robustness in terms of extreme load and helps application administrators to determine if the system will perform sufficiently if the current load goes well above the expected maximum.

   As a system becomes overloaded, users experience slower response times while they "stand in line"
   waiting for others to be processed.
   So average response times and response times at the 95th percentile 
   are defined to set maximum allowable levels.

   <amp-youtube data-videoid="I6EzGV0GE2A" 
   layout="responsive" width="480" height="270">
   </amp-youtube>

3. How long can the system run continuously?

   Sometimes programs "leak" memory and require more and more memory until it runs out.

   <strong>"Soak tests"</strong> run the system over several hours or days to 
   determine if a system is exposed to that risk.

   These are also called "longevity tests" or "endurance tests" done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation, i.e. to ensure that the throughput and/or response times after some long period of sustained activity are as good as or better than at the beginning of the test. It essentially involves applying a significant load to a system for an extended, significant period of time.
   <a href="#[1]">*</a>

   The goal is to discover how the system behaves under sustained use.


4. Can the system handle momentary sudden jumps in demand?

   <strong>"Spike tests"</strong> are included within soak tests to see whether the system has enough 
   "buffer" capacity to process sudden abnormal spikes in requests.

   Spike testing is done by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. 

   Such tests are among "resiliency tests" that reveal conditions which cause performance 
   will suffer or the system will fail.

   <strong>Performance Engineering</strong> works to fix the underlying coding or configurations
   to ensure resiliency.

5. How many additional servers need to be added to meet new demand?

   "Scalability tests" add more servers and/or larger servers, or remove them.

   Usually, doubling the amount of hardware does not double the rate of processing because of
   inefficiencies added, such as additional communications necessary.
   Planning and testing reveal limits in supporting infrastructure such as the capacity 
   in networking equipment.

6. Are configuration settings set at optimal levels?

   "Configuration tests" determine the effects of configuration changes on the system's performance and behaviour. 

   These are also called "tuning" runs.

   The objective is savings from less hardware being required to handle the same capacity.

   Indiviual responses from the database, application server, etc. are monitored during test runs in order to identify <strong>bottlenecks</strong> in the application software and the hardware the software is installed on.




## For project leads

Due to variations in momentary network speeds and other aspects that might vary during test runs,
test runs are often repeated before a <strong>baseline</strong> is selected as the basis for comparison
against alternative runs.


The set of performance tests conducted answer these questions:<a href="#[1]">*</a>

0. What is the performance test scope? What subsystems, interfaces, components, etc. are in and out of scope for this test?

0. For the user interfaces (UIs) involved, how many <strong>concurrent users</strong> are expected for each (specify peak vs. nominal)?

0. What does the target hardware under test? (specify all server and network appliance configurations)?

0. What is the Application Workload Mix of each system component? (for example: 20% log-in, 40% search, 30% item select, 10% checkout).

0. What is the Workload Mix? [Multiple workloads may be simulated in a single performance test (for example: 30% Workload A, 20% Workload B, 50% Workload C).

0. What are the time requirements for any/all back-end batch processes (specify peak vs. nominal)?


## References

<a name="[1]"></a>
https://en.wikipedia.org/wiki/Software_performance_testing

https://wilsonmar.github.io/perftest/