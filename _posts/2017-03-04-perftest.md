---
layout: post
title: "Agile Performance Testing"
excerpt: "Why do performance testing? And how to do it in an Agile way"
tags: [perftest]
shorturl: "https://goo.gl/wNVxWD"
comments: true
---
<i>{{ page.excerpt }}</i>
<hr />

{% include _toc.html %}

This is about the economics of performance testing and performance engineering work.

## For executives

Performance testing is done to:

   * ensure productivity of employees 
   * prevent losing sales from vistors abandoning interactions
   * avoid damaged to the company's reputation 

## For strategic program managers

ANALOGY: Spending money on performance testing is like companies paying for pre-employment drug tests.
All employers inconvenience new hires with 
drug testing even though the vast majority of employees don't use drugs because
the consequences of that one bad apple potentially devastating trust among employees.

Just as there are several types of drugs potential employees can be using, 
there are different <strong>performance risks</strong> that may undermine enterprise systems.

Various types of performance testing address different concerns:

1. Where can bad programming "hang" the system?

   For the same reason as 100% of candidates are made to take drug tests,
   the more <strong>coverage</strong> of user functionality in performance tests, 
   the more likely we are to find hidden issues.

   "Performance tests" are done to identify how quickly servers respond
   to requests from customers and employees. The measurement is called
   <strong>response time</strong>, expressed in seconds or thousands of a second (milliseconds).

   The "Shift-left" movement to catch performance problems earlier involves
   watching out for slow performance while developers create the system.
   
   As more and more processing moves to software on devices rather than on servers,
   many organizations now also measure how much time client programs (operated by end-users) 
   take to "render" data obtained from servers.


2. What is the <strong>capacity</strong> of the system?<br />
   What is the highest <strong>rate</strong> the system processes business transactions?

   Some servers work until a certain level of work burns them out.
   It is always helpful to have a statement of the likely peak number of users that might be expected to use the system at peak times. 

   <strong>Peak</strong> demand in a system is defined practically by the
   <strong>number of users</strong> who are actively using the system at the same time
   (concurrently).

   "Load tests" are conducted to identify the rate business transactions can be processed without failure
   at various levels of expected load (demand from users).

   "Stress tests" reveals how much servers can handle before they run out of 
   memory, processing power, disk space, or other resource needed.

   Stress testing is normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system's robustness in terms of extreme load and helps application administrators to determine if the system will perform sufficiently if the current load goes well above the expected maximum.

   As a system becomes overloaded, users experience slower response times while they "stand in line"
   waiting for others to be processed.
   So average response times and response times at the 95th percentile 
   are defined to set maximum allowable levels.

   <amp-youtube data-videoid="I6EzGV0GE2A" 
   layout="responsive" width="480" height="270">
   </amp-youtube><br /><br />

3. How long can the system run continuously?

   Sometimes programs "leak" memory and require more and more memory until it runs out.

   <strong>"Soak tests"</strong> run the system over several hours or days to 
   determine if a system is exposed to that risk.
   These are also called "longevity tests" or "endurance tests" done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation, i.e. to ensure that the throughput and/or response times after some long period of sustained activity are as good as or better than at the beginning of the test. It essentially involves applying a significant load to a system for an extended, significant period of time.
   <a href="#[1]">*</a>

   The goal is to discover how the system behaves under sustained use.


4. Can the system handle momentary sudden jumps in demand?

   <strong>"Spike tests"</strong> are included within soak tests to see whether the system has enough 
   "buffer" capacity to process sudden abnormal spikes in requests.

   Spike testing is done by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. 

   Such tests are among a set of tests which reveal conditions that can cause performance 
   to suffer or make the system fail.

   <strong>Performance Engineering</strong> works to fix the underlying coding or configurations
   to ensure resiliency.

5. How many additional servers should be added (or removed) to meet anticipated demand?

   "Scalability tests" add more servers and/or larger servers, or remove them.

   Usually, doubling the amount of hardware does not double the rate of processing because of
   inefficiencies added, such as additional communications necessary.
   Planning and testing reveal limits in supporting infrastructure such as the capacity 
   in networking equipment.

6. Are configuration settings set at optimal levels?

   "Configuration tests" determine the effects of configuration changes on the system's performance and behaviour. 

   These are also called "tuning" runs.

   The objective is savings from less hardware being required to handle the same capacity.

   Indiviual responses from the database, application server, etc. are monitored during test runs in order to identify <strong>bottlenecks</strong> in the application software and the hardware the software is installed on.

7. Will growth in amount of data over time affect performance?

   "Data volumn testing" determines wether the performance of the application is affected by the amount (volume) of data being handled by the application. 
   The structure of data may not provide adequate performance as
   databases grow over time with historical information. 

   Performing such testing requires a huge volume of data needs to be artifically created in the database. The enormity of such a task is beyond most organizations.


## For project leads

Due to variations in momentary network speeds and other aspects that might vary during test runs,
test runs are often repeated before a <strong>baseline</strong> is selected as the basis for comparison
against alternative runs.

Performance test plans answer these questions:<a href="#[1]">*</a>

0. What is the performance test scope? What subsystems, interfaces, components, etc. are in and out of scope for this test?

0. How many <strong>concurrent users</strong> are expected for each user interfaces (UIs) involved?

   Specify peak vs. nominal.

0. What are the time requirements for any/all back-end batch processes?

   Specify peak vs. nominal.

0. What is the target hardware under test? (specify all server and network appliance configurations)?

0. What is the Application Workload Mix by each system component? 

   For example: Workload A consists of 20% log-in, 40% search, 30% item select, 10% checkout, etc..

0. What is the Workload Mix? [Multiple workloads may be simulated in a single performance test 

   For example: 30% Workload A, 20% Workload B, 50% Workload C.



### Enablement questions

Engage with us to answer these questions:

1. How to ensure dependencies are ready before sprint work needs them ...

   ... instead of scrambling to fix things blocking progress.

2. How to automatically identify performance issues (such as memory leaks) as part of System Demos at the end of sprints ...

   ... instead of waiting until it's a hassle and more expensive to make changes.

   A central tenent of Agile is to measure progress by workable products of most value to customers.

   Is a product really "done" when there are unassessed hazards?

   DEFINITIONS: Risk is the chance of loss. Peril is the direct cause of the loss. If a house burns down, then fire is the peril. A hazard is anything that either causes or increases the likelihood of a loss. 
   For instance, gas furnaces are a hazard for carbon monoxide poisoning.
   In computer systems: lack of availability for customer usage is a risk. 
   Application failures and limited capacity are perils. 
   Memory leaks are hazards for application failure.

3. How to integrate work among groups working sequential shifts ...

   ... instead of limiting capacity to a single cabal of workers.



## For Technical Team Members

Engage with us to answer these questions:

1. How to ensure services are available before sprint work needs them ...

   ... instead of scrambling to fix things blocking progress.

   There is a way to frequently "ping" dependencies in order to measure patterns in their availability.

2. How to automatically identify performance issues (such as memory leaks) as part of System Demos at the end of sprints ...

   We gradually build system-level load test assets over time
   by accumulating micro-benchmarks at the component level as developers work.

   Also, have separate parallel "hardening" sprints that occur after System Demos
   to conduct performance testing:<br />
   <a target="_blank" title="by Prakash Mallappa Pujar" href="https://cloud.githubusercontent.com/assets/300046/24324628/2b35fe50-1160-11e7-9f15-cbbbf3b94536.jpeg">*</a>
   <a target="_blank" title="by Prakash Mallappa Pujar" href="https://www.scrumalliance.org/community/articles/2013/2013-may/agile-performance-testing-an-experimental-approach">
   <img alt="agile_performance_testing_image prakash_mallappa_pujar__2" src="https://cloud.githubusercontent.com/assets/300046/24324628/2b35fe50-1160-11e7-9f15-cbbbf3b94536.jpeg"><br />(Click for full screen pop-up)</a>

3. How to integrate work among groups working sequential shifts ...

   ... instead of limiting capacity to a single cabal of workers.

   We use "smart" scheduling systems and advanced yet lean communication protocols among members.

4. How to have automation assist in generating scripts to impose potential loads ...

   ... instead of responding to change by manually adding correlations and verifications again after each change.

   We have "functional" test scripts run to generate load test scripts using advanced software that
   pulls in previously defined correlations and verifications.

5. How to tune configuration settings for lower costs and resiliency during sprints ...

   ... instead of letting default settings wreck havoc on servers.

   We can run tuning runs overnight when we can assess results automatically.

6. How to provide Production Operations predictive triggers for action ...

   ... instead of leaving others to figure it out on their own.

   We identify conditions (such as memory per user) that are evident 
   before the system reaches during artificially induced load levels.

The System enablement team provides automation tools and step-by-step procedural training on each of the above.

Click here for an overview of the answers to the above.



## References

Agile practioners are increasing questioning the efficacy of an approach to performance testing
crammed into

   and whether such teams are really working together with feature development teams.

says:

If performance testing is part of the Definition of Done, then that testing can happen only during the last few days of the sprint cycle. The result:

   * Development activity stops before the sprint ends. Waterfall within Scrum?

   * Performance testing is done only for individual features by feature sprint teams. Performance testing is not done at the system level, integrating all the features developed in the sprint. Building performance risk at product level?

   * If feature development continues, these features aren't included in the performance testing of the same sprint. Not all the features developed in the sprint are tested. Conflict in meeting the Definition of Done?

   * Possible chance that all team members aren't 100 percent effective throughout the sprint. Team's velocity fluctuates.

- See more at: https://www.scrumalliance.org/community/articles/2013/2013-may/agile-performance-testing-an-experimental-approach#sthash.wcswOPNl.dpuf


<a name="[1]"></a>
https://en.wikipedia.org/wiki/Software_performance_testing

https://wilsonmar.github.io/perftest/
