---
layout: post
title: "NGNIX"
excerpt: "The kick-ass secure front-end web server proxy"
tags: [web, performance]
image:
# pic silver robot white skin handshake 1900x500
  feature: https://cloud.githubusercontent.com/assets/300046/14622149/306629f0-0585-11e6-961a-dc8f60dadbf6.jpg
  credit: 
  creditlink: 
comments: true
---
<i>{{ page.excerpt }}</i>

{% include _toc.html %}

This article contains my notes about installing and using NGINX 
(pronounced "engine-x").

The NGINX web server is more efficient at responding to static requests
than IIS and Apache Tomcat.
This is because provides high concurrency, high
performance and low memory usage, using an "event-driven" approach.

It serves as a cache, provides load balancing of incoming traffic
as a "reverse proxy".

NGINX negotiates HTTPS traffic
so downstream servers can use plain
HTTP traffic without the encryption overhead.
This is important for companies who want to keep TLS/SSL certificate handling
on only one set of servers by a small set of people.

NGINX also handles SMTP, POP3, IMAP, and other protocols.


## Install #

0. On Mac:

   brew install nginx

   The response (redacted):

   <pre>
==> Installing dependencies for nginx: openssl@1.1
==> Installing nginx dependency: openssl@1.1
==> Caveats
A CA file has been bootstrapped using certificates from the system
keychain. To add additional certificates, place .pem files in
  /usr/local/etc/openssl@1.1/certs
&nbsp;
and run
  /usr/local/opt/openssl@1.1/bin/c_rehash
&nbsp;
This formula is keg-only, which means it was not symlinked into /usr/local.
&nbsp;
Apple has deprecated use of OpenSSL in favor of its own TLS and crypto libraries
&nbsp;
Generally there are no consequences of this for you. If you build your
own software and it requires this formula, you'll need to add to your
build variables:
&nbsp;
    LDFLAGS:  -L/usr/local/opt/openssl@1.1/lib
    CPPFLAGS: -I/usr/local/opt/openssl@1.1/include
    PKG_CONFIG_PATH: /usr/local/opt/openssl@1.1/lib/pkgconfig
&nbsp;
==> Summary
üç∫  /usr/local/Cellar/openssl@1.1/1.1.0b: 6,222 files, 15.3M
==> Installing nginx
==> Downloading https://homebrew.bintray.com/bottles/nginx-1.10.2_1.sierra.bottl
######################################################################## 100.0%
==> Pouring nginx-1.10.2_1.sierra.bottle.tar.gz
==> Caveats
Docroot is: /usr/local/var/www
&nbsp;
The default port has been set in /usr/local/etc/nginx/nginx.conf to 8080 so that
nginx can run without sudo.
&nbsp;
nginx will load all files in <strong>/usr/local/etc/nginx/servers/</strong>.
&nbsp;
To have launchd start nginx now and restart at login:
  brew services start nginx
Or, if you don't want/need a background service you can just run:
  nginx
==> Summary
üç∫  /usr/local/Cellar/nginx/1.10.2_1: 8 files, 979.5K   
   </pre>

   Alternately, on Linux:

   sudo apt-get update

   sudo apt-get install nginx

   ### OpenSSL version:

0. Run:

   <tt><strong>
   openssl version
   </strong></tt>

   As long as the version is beyond 1.0.1p

   <pre>
   OpenSSL 1.0.2g  1 Mar 2016
   </pre>

   The version for Heartbleed was 1.0.1p
   


   ### Bring it up 

0. Run:

   <tt><strong>
   nginx
   </strong></tt>

   The prompt returns.

0. Notice it runs under the bash shell, not as a separate process:

   <tt><strong>
   ps
   </strong></tt>
   
0. In a browser:

   <tt><strong>
   localhost:8080
   </strong></tt>
   
   ![nginx hello 527x208-100kb](https://cloud.githubusercontent.com/assets/14143059/19688633/c4957de6-9a87-11e6-96e7-d483e8d4ef17.jpg)

   ### Examine config #

0. List the files installed (from path output during Mac installation above):

   <tt><strong>
   ls /usr/local/etc/nginx/servers/
   </strong></tt>

   The response:

   <pre>
conf.d          mime.types        proxy_params    uwsgi_params
fastcgi_params  naxsi_core.rules  scgi_params     win-utf
koi-utf         naxsi.rules       sites-available
koi-win         nginx.conf        sites-enabled
   </pre>   

   On Linux:

   <pre>
   cd /etc/nginx/
   </pre>

   ### Examine config #

0. How mancy cores does your machine have?

   The cross-platform approach: 

   <pre><strong>   
   python -c 'import multiprocessing as mp; print(mp.cpu_count())'
   </strong></pre>

   On a Mac, use a utility from Apple:

   <tt><strong>
   sysctl -n hw.ncpu<br />
   sysctl hw.physicalcpu
   </strong></tt>

   The response is 8 <strong>logical</strong> cores
   from 2 physical cores from  system_profiler SPHardwareDataType .


   On Linux:

   <tt><strong>
   grep ^processor /proc/cpuinfo | wc -l
   </strong></tt>

## Configuration #

0. Examine the default configuration file 
   (substituting vim with atom or whatever text editor you prefer):

   <tt><strong>
   vim nginx.conf
   </strong></tt>

   Directives are defined in <br />
   <a target="_blank" href="http://nginx.org/en/docs/http/ngx_http_core_module.html">
   http://nginx.org/en/docs/http/ngx_http_core_module.html</a>

   ### Processes 

   For `worker_processes` specify one for each core on the machine being used.
   The default is a 4-core server.

   Each worker can handle thousands of concurrent connections
   asynchronously in a single thread.

   ### Connections

   The `worker_connections` default of 768 needs to be adjusted based on 
   what a server can handle based on the set of applications running on it.

   Since separate connections are used for incoming and outgoing,
   each end-user browser uses at least two connections.

0. Check your machine's connection limit:

   <tt><strong>
   ulimit -n
   </strong></tt>

   On my Mac, it's 256.

   On a small Linux droplet (512 MB), it may be 1024.

   Have a script do this upon start-up.

   ### Keepalive

   The `keepalive_timeout` default of 65 also needs to be adjusted based on 
   the remoteness of users from servers.
   For example, if the majority of users are in South Africa are accessing a server
   in Menlo Park, then you'll likely need a longer one.

   ### Buffer sizes #

   If you encounter a HTTP 413 "Rquest Entity Too Large" error,
   adjust these:

   `client_body_buffer_size 10K;`
   defines the max POST actions received (typically HTML FORMs).

   `client_max_body_size 8m;`

   `client_header_buffer_size 1K;`
   usually need to be increased if the header contains a lot of info.
   64K is a good size. 

   The companion is to up to `4 64K`
   `large_client_header_buffers 2 1K;`


   ### Gzip  

   If `gzip_comp_level` is too high, the server wastes CPU cycles.

   <pre>
   location /api {
     expires 10m;
   }
   </pre>

   ### Chef replace #

   Chef knows to look in the file and replaces values from a file such as this:

   <pre>
node.set[‚Äônginx‚Äô][‚Äòworker_connections‚Äô] = 8192
node.set[‚Äônginx‚Äô][‚Äòworker_rlimit_nofile‚Äô] = 32768
node.set[‚Äônginx‚Äô][‚Äòevent‚Äô] = ‚Äòepoll'
node.set[‚Äônginx‚Äô][‚Äòclient_max_body_size‚Äô] = ‚Äò4m'
   </pre>



### sites-enabled defaults #

0. Define a <strong>defaults</strong> file within site-enabled folder:

   <tt><strong>
   cd sites-enabled<br />
   sudo vim defaults
   </strong></tt>

   Define nodes (projects), all listening on port 80:

   <pre>
upstream project {
  server 22.22.22.2:3000;
#  server 22.22.22.3:3000;
# server 22.22.22.4:3000;
}
server {
   listen 80;

   location / {
     proxy_pass https://project;
   }

  # Cache static assets 7 days (168/24):
  location ~* \.(ico|gif|jpe?g|png|mp4|mp3|svg|css|js)$ {
    exprires 168h;
  }
}
   </pre>

   The `http://project` forward incoming requests among ip addresses 
   defined under `project` (app container). 
   This load balances.

   NOTE: HTTP 304 is issued for requests proxied.

   ### With Docker:

   Use a resolver server:

   <pre>
server {
   listen 80;

   resolver 127.0.0.11 valid=5s;
   set $upstream http://project;
   location / {
     proxy_pass $upstream;
   }

  # Cache static assets 7 days (168/24):
  location ~* \.(ico|gif|jpe?g|png|mp4|mp3|svg|css|js)$ {
    exprires 168h;
  }
}
   </pre>

   Remember for 5 seconds.

   ### Which ports are listening?

   <tt><strong>
   lsof -nP +c 15 | grep LISTEN
   </strong></tt>

   Drag your Terminal window wider to remove word-wrap.


## Multiple upstreams & rewrite rules

   <a target="_blank" href="https://www.youtube.com/watch?v=A2NOziRYh7U">
   Load Balancing a Dynamic Infrastructure with NGINX, Chef, and Consul</a>
   by Kevin Reedy (of Belly) presented this at ngix.conf 2014:

   Using Chef's templating:

   <pre>
upstream api {
  least_conn;
  &LT;$ @servers.each do |server\ %>
  server &LT;%= server['ipaddress %>:8080;
  &LT;% end %>
}
upstream homepage {
  least_conn;
  server 10.0.0.201:8080;
  server 10.0.0.202:8080;
}
server {
  listen: 80;
  server_name api.bellycard.com;
  location / {
    proxy_pass http://api;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_add;
    proxy_set_header X-Forwarded-Proto https;
  }
}
server {
  listen: 80;
  server_name www.bellycard.com;
  location / {
    proxy_pass http://homepage;
  }
  location /api {
    proxy_pass http://api;
  }
}
   </pre>

   ### SSL/TLS

   <pre>
# Usign a certificate created for multiple domains:

   # Certificate chain: 
   ssl_certificate /usr/local/etc/openssl@1.1/certs/signed_cert_plus_intermediates.crt;

   # Private key:
   ssl_certificate_key /path/to/private.key;

   # 50 megabyte Session caching for faster connections from existing clients:
   ssl_session_timeout 1d;
   ssl_session_cache shared:SSL:50m;

   ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
   ssl_ciphers EECHD+CHACHA20:EECDH+AES128:RSA+AES128:EECD+AES256:RSA+AES256:EECD+3DES:RSA+3DES:!MD5;
   ssl_prefer_server_ciphers on;

   # HSTS (Hypertext Strick Transport Security) to always reach using HTTPS 
   # which prevents hijacking of HTTP for 6 months:
   # (requires ngx_http_headers_module)
   # add_header Strict-Transport-Security max-age=15768000;

   # Session tickets for Chrome & Firefox users:
   # Generate a random 48-byte file.

   # Fetch OCSP (Online Certificate Status Protocol) records from 
   # URL in ssl_certs and to pre-fetch OSCP response, to save a round trip
   # (30A% faster) by providing file from CA:
   ssl_stapling on;
   ssl_stapling_verify on;
   ssl_trusted_certificate /usr/local/etc/openssl@1.1/certs/root_CA_cert_plus_intermediates.crt;

server {
   listen 443 ssl;
   server_name   www.example.com;
...
}
server {
   listen 443 ssl;
   server_name   www.example.org;
}
...
   </pre>

   ### Back-end encryption #

   <pre>
http {
  server {
    proxy_ssl_protocols TLSv1 TLSv1.1 TLSv1.2;

    proxy_ssl_ciphers EECHD+CHACHA20:EECDH+AES128:RSA+AES128:EECD+AES256:RSA+AES256:EECD+3DES:RSA+3DES:!MD5;

    # Internal or public: On Ubuntu:
    proxy_ssl_trusted_certificate /etc/ssl/certs/trusted_ca_cert.crt;

    proxy_ssl_verify on;
    proxy_ssl_verify_depth 2;
    proxy_ssl_session_reuse on;
...
   </pre>


## Start service #

0. Define a <strong>defaults</strong> file within site-enabled folder:

   <tt><strong>
   sudo service nginx start
   </strong></tt>


   ### Stress test #

0. Use Apache bench to run 1000 requests 40 concurrent (at a time):

   <tt><strong>
   ab -c 40 -n 1000 http://22.22.22.4/
   </strong></tt>

   1000 / 40 = 25 / .5 seconds = 5 seconds to complete benchmark run.


   ### Add servers 

0. PROTIP: When more are needed, edit the file and reload.

   <tt><strong>
   sudo service nginx reload
   </strong></tt>

## Docker

<a target="_blank" href="https://www.youtube.com/watch?v=HJ9bECmuwKo">
   https://www.youtube.com/watch?v=HJ9bECmuwKo</a>
   July 2016
   by Quentin Stafford-Fraser
   uses a dockerfile:

   <pre>
   FROM nginx:alpine
   COPY index.html /usr/share/nginx/html
   </pre>

The docker-compose.yml file:

   <pre>
version: '2'
services:
  app:
    build: app
  proxy:
    build: proxy
    ports:
    - "80:80"
   </pre>

   Version 2 means Docker uses its DNS server so a link is not needed here.

   The build is to the proxy directory.

   "80:80" binds from local host to port on proxy.

0. Build

   <tt><strong>
   docker-compose build
   </strong></tt>

0. Up

   <tt><strong>
   docker-compose up
   </strong></tt>

0. Verify

   <tt><strong>
   docker ps
   </strong></tt>

0. View in proxy:

   <tt><strong>
   docker-compose exec proxy sh
   </strong></tt>

   List IPs addressed by proxy:

   <tt><strong>
   nslookup app
   </strong></tt>

0. Scale

   <tt><strong>
   docker-compose scale -h
   </strong></tt>

   To fire up 4 instances of app services and 2 workers:

   <tt><strong>
   docker-compose scale app=4 worker=2
   </strong></tt>

<a target="_blank" href="https://www.youtube.com/watch?v=6uucWLPcAPY">
Interconnecting containers at scale with NGINX</a>
by Sarah Novotny, Technical Evangelist, NGINX



## Verify security

When deployed to a public site:

   https://ssllabs.com

See https://blog.qualys.com/ssllabs/2013/06/25/ssl-labs-deploying-forward-secrecy

## Videos #

<a target="_blank" href="https://www.youtube.com/channel/UCy6gt7XvGJ3AGpSon2pS4nQ">
  On the NGINX YouTube channel</a>:


* <a target="_blank" href="https://www.youtube.com/watch?v=dsTub1_4Upg">
   NGINX + https 101 The Basics & Getting Started</a>
   at nginx.conf 2015 Sep 22-24
   by Nick Sullivan @grittygrease
   from Cloudflare

Others:

* <a target="_blank" href="https://www.youtube.com/watch?v=6oXn5blsCWM">
   What is Nginx | Ngnix basic Video Tutorial | DevOps Tutorial For Beginners | Edureka</a>

* https://www.youtube.com/watch?v=YHcRzDKo6qU
   NGINX -- The Web Server You Might Actually Like
   by International PHP Conference
