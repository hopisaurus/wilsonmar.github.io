---
layout: post
title: "Perftest (Performance Testing and Engineering)"
excerpt: "A high-level explanation of Why do it? And how"
tags: [perftest]
shorturl: "https://goo.gl/ZkKQrs"
comments: true
---
<i>{{ page.excerpt }}</i>
<hr />

{% include _toc.html %}

This is about the economics of performance testing and performance engineering work.

## For executives

Performance testing is done to:

   * ensure productivity of employees 
   * prevent losing sales from vistors abandoning interactions
   * avoid damaged to the company's reputation 

## For strategic program managers

ANALOGY: Spending money on performance testing is like companies paying for pre-employment drug tests.
All employers inconvenience new hires with 
drug testing even though the vast majority of employees don't use drugs because
the consequences of that one bad apple potentially devastating trust among employees.

Just as there are several types of drugs potential employees can be using, 
there are different <strong>performance risks</strong> that may undermine enterprise systems.

Various types of performance testing address different concerns:

1. <strong>Where can bad programming "hang" the system?</strong>

   For the same reason as 100% of candidates are made to take drug tests,
   the more <strong>coverage</strong> of user functionality in performance tests, 
   the more likely we are to find hidden issues.

   "Performance tests" are done to identify how quickly servers respond
   to requests from customers and employees. The measurement is called
   <strong>response time</strong>, expressed in seconds or thousands of a second (milliseconds).

   The "Shift-left" movement to catch performance problems earlier involves
   watching out for slow performance while developers create the system.
   
   As more and more processing moves to software on devices rather than on servers,
   many organizations now also measure how much time client programs (operated by end-users) 
   take to "render" data obtained from servers.


2. **What is the <strong>capacity</strong> of the system?<br />
   What is the highest rate the system processes business transactions?**

   Some servers work until a certain level of work burns them out.
   It is always helpful to have a statement of the likely peak number of users that might be expected to use the system at peak times. 

   <strong>Peak</strong> demand in a system is defined practically by the
   <strong>number of users</strong> who are actively using the system at the same time
   (concurrently).

   "Load tests" are conducted to identify the rate business transactions can be processed without failure
   at various levels of expected load (demand from users).

   "Stress tests" reveals how much servers can handle before they run out of 
   memory, processing power, disk space, or other resource needed.

   Stress testing is normally used to understand the upper limits of capacity within the system. This kind of test is done to determine the system's robustness in terms of extreme load and helps application administrators to determine if the system will perform sufficiently if the current load goes well above the expected maximum.

   As a system becomes overloaded, users experience slower response times while they "stand in line"
   waiting for others to be processed.
   So average response times and response times at the 95th percentile 
   are defined to set maximum allowable levels.

   <a name="Predictive"></a>
   <amp-youtube data-videoid="I6EzGV0GE2A" 
   layout="responsive" width="480" height="270">
   </amp-youtube><br /><br />

3. **How long can the system run continuously?**

   Sometimes programs "leak" memory and require more and more memory until it runs out.

   <strong>"Soak tests"</strong> run the system over several hours or days to 
   determine if a system is exposed to that risk.
   These are also called "longevity tests" or "endurance tests" done to determine if the system can sustain the continuous expected load. During soak tests, memory utilization is monitored to detect potential leaks. Also important, but often overlooked is performance degradation, i.e. to ensure that the throughput and/or response times after some long period of sustained activity are as good as or better than at the beginning of the test. It essentially involves applying a significant load to a system for an extended, significant period of time.
   <a href="#[1]">*</a>

   The goal is to discover how the system behaves under sustained use.


4. **Can the system handle momentary sudden jumps in demand?**

   <strong>"Spike tests"</strong> are included within soak tests to see whether the system has enough 
   "buffer" capacity to process sudden abnormal spikes in requests.

   Spike testing is done by suddenly increasing or decreasing the load generated by a very large number of users, and observing the behaviour of the system. 

   Such tests are among a set of tests which reveal conditions that can cause performance 
   to suffer or make the system fail.

   <strong>Performance Engineering</strong> works to fix the underlying coding or configurations
   to ensure resiliency.

5. **How many additional servers should be added (or removed) to meet anticipated demand?**

   "Scalability tests" add more servers and/or larger servers, or remove them.

   Usually, doubling the amount of hardware does not double the rate of processing because of
   inefficiencies added, such as additional communications necessary.
   Planning and testing reveal limits in supporting infrastructure such as the capacity 
   in networking equipment.

6. **Are configuration settings set at optimal levels?**

   "Configuration tests" determine the effects of configuration changes on the system's performance and behaviour. 

   These are also called "tuning" runs.

   The objective is savings from less hardware being required to handle the same capacity.

   Indiviual responses from the database, application server, etc. are monitored during test runs in order to identify <strong>bottlenecks</strong> in the application software and the hardware the software is installed on.

7. **Will growth in amount of data over time affect performance?**

   "Data volumn testing" determines wether the performance of the application is affected by the amount (volume) of data being handled by the application. 
   The structure of data may not provide adequate performance as
   databases grow over time with historical information. 

   Performing such testing requires a huge volume of data needs to be artifically created in the database. The enormity of such a task is beyond most organizations.


## For project leads

Due to variations in momentary network speeds and other aspects that might vary during test runs,
test runs are often repeated before a <strong>baseline</strong> is selected as the basis for comparison
against alternative runs.

Performance test plans answer these questions:<a href="#[1]">*</a>

0. What is the performance test scope? What subsystems, interfaces, components, etc. are in and out of scope for the test?

0. How many <strong>concurrent users</strong> are expected for each user interfaces (UIs) involved?

   Specify peak vs. nominal.

0. What are the time requirements for any/all back-end batch processes?

   Specify peak vs. nominal.

0. What is the target hardware under test? (specify all server and network appliance configurations)?

0. What is the Application Workload Mix by each system component? 

   For example: Workload A consists of 20% log-in, 40% search, 30% item select, 10% checkout, etc..

0. What is the Workload Mix? [Multiple workloads may be simulated in a single performance test 

   For example: 30% Workload A, 20% Workload B, 50% Workload C.




<a name="[1]"></a>
https://en.wikipedia.org/wiki/Software_performance_testing

https://wilsonmar.github.io/perftest/
